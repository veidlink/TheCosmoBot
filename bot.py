import json
import os
import uuid
import re
import pandas as pd
import time
import logging                                         
import torch
import requests
import asyncio                                
from bs4 import BeautifulSoup
from PIL import Image
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

from aiogram.types import ReplyKeyboardMarkup, KeyboardButton, ReplyKeyboardRemove, InlineKeyboardMarkup, InlineKeyboardButton
from aiogram import Bot, Dispatcher, types
from aiogram.dispatcher import FSMContext
from aiogram.dispatcher.filters.state import State, StatesGroup
from aiogram.contrib.fsm_storage.memory import MemoryStorage

class UserStates(StatesGroup):
    AwaitingStart = State()   # –ù–æ–≤–æ–µ –Ω–∞—á–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
    Start = State()            # –ù–∞—á–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
    WaitingForPhoto = State()  # –û–∂–∏–¥–∞–Ω–∏–µ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏
    AnalyzingPhoto = State()   # –ê–Ω–∞–ª–∏–∑ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏
    ShowingResults = State()   # –ü–æ–∫–∞–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    ParsingReviews = State()   # –ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç–∑—ã–≤–æ–≤

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –±–æ—Ç–∞
    
# Read the configuration file
with open('config.json', 'r') as config_file:
    config_data = json.load(config_file)

# Access private info
TOKEN = config_data.get('TOKEN')
DEVICE = 'cpu' # CHANGE TO cpu IF NEEDED

model = torch.hub.load(
'yolov5',  
'custom',  
path='datasets/weights/best_weights.pt', # –ø—É—Ç—å –∫ –Ω–∞—à–∏–º –≤–µ—Å–∞–º
source='local',
force_reload=True 
);

model.to(DEVICE)
model.conf = 0.2 # —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–æ—Ä–æ–≥
# detected_labels = []

tokenizer = AutoTokenizer.from_pretrained("SiberiaSoft/SiberianFredT5-instructor")
summarizer = AutoModelForSeq2SeqLM.from_pretrained("SiberiaSoft/SiberianFredT5-instructor")
summarizer.to(DEVICE)

# async def generate(prompt):
#     data = tokenizer('<SC6>' + prompt + '\n–û—Ç–≤–µ—Ç: <extra_id_0>', return_tensors="pt")
#     data = {k: v.to(DEVICE) for k, v in data.items()}
#     output_ids = summarizer.generate(
#         **data,  do_sample=True, temperature=0.2, max_new_tokens=512, top_p=0.95, top_k=5, repetition_penalty=1.03, no_repeat_ngram_size=2
#     )[0]
#     out = tokenizer.decode(output_ids.tolist())
#     out = out.replace("<s>","").replace("</s>","")
#     cleaned_text = re.sub(r'<[^>]+>', '', out)
#     return cleaned_text

def generate(prompt):
    data = tokenizer('<SC6>' + prompt + '\n–û—Ç–≤–µ—Ç: <extra_id_0>', return_tensors="pt")
    data = {k: v.to(DEVICE) for k, v in data.items()}
    output_ids = summarizer.generate(
        **data,  do_sample=True, temperature=0.2, max_new_tokens=512, top_p=0.95, top_k=5, repetition_penalty=1.03, no_repeat_ngram_size=2
    )[0]
    out = tokenizer.decode(output_ids.tolist())
    out = out.replace("<s>","").replace("</s>","")
    cleaned_text = re.sub(r'<[^>]+>', '', out)
    return cleaned_text

# –ú–∞–ø–ø–∏–Ω–≥ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –Ω–∞ –ø–æ–¥—Ö–æ–¥—è—â–∏–µ —Å–º–∞–π–ª–∏–∫–∏
category_smileys = {
    "–∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ç": "üòÑ",
    "–∫—Ä–µ–º –∏–ª–∏ –ª–æ—Å—å–æ–Ω": "üòä",
    "–ª–æ—Å—å–æ–Ω": "üåø",
    "—Å–∫—Ä–∞–±": "üåä",
    "–≥–µ–ª—å": "üòé",
    "–º–∞—Å–∫–∞": "üò∑",
    "—Å—ã–≤–æ—Ä–æ—Ç–∫–∞": "üß¥",
    "—Ç–æ–Ω–∏–∫": "üåº",
    "–∫–∏—Å–ª–æ—Ç–∞": "üçã",
    "–ø–∏–ª–∏–Ω–≥": "üßº"
}

bot = Bot(token=TOKEN)
storage = MemoryStorage()
dp = Dispatcher(bot, storage=storage)

logging.basicConfig(level=logging.INFO)

@dp.message_handler(commands=['start'], state="*")
async def start_handler(message: types.Message, state: FSMContext):
    await UserStates.Start.set()
    user_first_name = message.from_user.first_name
    user_name = message.from_user.username
    logging.info(f'{user_first_name}, {user_name}, {time.asctime()}')

    # –°–æ–∑–¥–∞–π—Ç–µ –∫–ª–∞–≤–∏–∞—Ç—É—Ä—É —Å –¥–≤—É–º—è –∫–Ω–æ–ø–∫–∞–º–∏
    markup = ReplyKeyboardMarkup(resize_keyboard=True, one_time_keyboard=True)
    button1 = KeyboardButton("–ß—Ç–æ —ç—Ç–æ –∑–∞ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ?ü¶Ñ")
    button2 = KeyboardButton("–•–æ—á—É —Å–∫–æ—Ä–µ–µ –≤–æ—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—èü§§")

    markup.add(button1, button2)
    # –°–æ–∑–¥–∞–π—Ç–µ –º–µ–Ω—é —Å –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–º–∏ –∫–Ω–æ–ø–∫–∞–º–∏
    menu_markup = InlineKeyboardMarkup()
    menu_button1 = InlineKeyboardButton("–û—Å–Ω–æ–≤–Ω–æ–µ –º–µ–Ω—é", callback_data="main_menu")
    menu_markup.add(menu_button1)

    # –û—Ç–ø—Ä–∞–≤—å—Ç–µ –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ —Å —Ñ–æ—Ç–æ –∏ –∫–ª–∞–≤–∏–∞—Ç—É—Ä–æ–π —Å –∫–Ω–æ–ø–∫–∞–º–∏
    photo = open('botik.png', 'rb')  # –ó–∞–º–µ–Ω–∏—Ç–µ 'path_to_your_image.jpg' –Ω–∞ –ø—É—Ç—å –∫ –≤–∞—à–µ–º—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é
    await bot.send_photo(
        chat_id=message.chat.id,
        photo=photo,
        caption='–ü—Ä–∏–≤–µ—Ç‚ù£Ô∏è\n\n–Ø —Ç–≤–æ–π –∫–æ—Å–º–µ—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–º–æ—â–Ω–∏–∫. –ß–µ–º –º–æ–≥—É –ø–æ–º–æ—á—å?',
        reply_markup=markup
    )
    photo.close()  # –ó–∞–∫—Ä—ã–≤–∞–µ–º —Ñ–∞–π–ª —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º

@dp.message_handler(lambda message: message.text == '–ß—Ç–æ —ç—Ç–æ –∑–∞ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ?ü¶Ñ', state=UserStates.Start)
async def app_info_handler(message: types.Message, state: FSMContext):
    # –°–æ–∑–¥–∞–π—Ç–µ –∫–ª–∞–≤–∏–∞—Ç—É—Ä—É —Å –∫–Ω–æ–ø–∫–æ–π "–•–æ—á—É —Å–∫–æ—Ä–µ–µ –≤–æ—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è" –ø–æ—Å–ª–µ –æ—Ç–≤–µ—Ç–∞
    markup = ReplyKeyboardMarkup(resize_keyboard=True, one_time_keyboard=True)
    button = KeyboardButton("–•–æ—á—É —Å–∫–æ—Ä–µ–µ –≤–æ—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—èü§§")
    markup.add(button)
    # –°–æ–∑–¥–∞–π—Ç–µ –º–µ–Ω—é —Å –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–º–∏ –∫–Ω–æ–ø–∫–∞–º–∏
    menu_markup = InlineKeyboardMarkup()
    menu_button1 = InlineKeyboardButton("–û—Å–Ω–æ–≤–Ω–æ–µ –º–µ–Ω—é", callback_data="main_menu")
    menu_markup.add(menu_button1)


    # –û—Ç–ø—Ä–∞–≤—å—Ç–µ –æ—Ç–≤–µ—Ç –ø–æ—Å–ª–µ –Ω–∞–∂–∞—Ç–∏—è –∫–Ω–æ–ø–∫–∏ "–ß—Ç–æ —ç—Ç–æ –∑–∞ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ?" —Å –∫–ª–∞–≤–∏–∞—Ç—É—Ä–æ–π —Å –∫–Ω–æ–ø–∫–æ–π
    await message.reply(
        "–Ø —Ç–≤–æ—Ä–µ–Ω–∏–µ –∫–æ–º–∞–Ω–¥—ã –ü–∏–∫—Å–µ–ª—å–Ω—ã–µ –¢–∏—Ç–∞–Ω—ã, —Å–æ–∑–¥–∞–Ω, —á—Ç–æ–±—ã –ø–æ–º–æ–≥–∞—Ç—å –ª—é–¥—è–º –æ–ø—Ä–µ–¥–µ–ª—è—Ç—å –ø—Ä–æ–±–ª–µ–º—ã –Ω–∞ –∫–æ–∂–µ –∏ –ø—Ä–∏–º–µ—Ä–Ω—É—é —Å—Ç–æ–∏–º–æ—Å—Ç—å –ª–µ—á–µ–Ω–∏—è. –û—á–µ–Ω—å –≤–∞–∂–Ω–æ, –ø–µ—Ä–µ–¥ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –æ–±—Ä–∞—Ç–∏—Ç—å—Å—è –∫ —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç—Éüò∑",
        reply_markup=markup
    )

    # await UserStates.Start.set()  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤ –Ω–∞—á–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ

@dp.message_handler(lambda message: message.text == '–•–æ—á—É —Å–∫–æ—Ä–µ–µ –≤–æ—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—èü§§', state=UserStates.Start)
async def start_over(message: types.Message, state: FSMContext):
    # –°–æ–∑–¥–∞–π—Ç–µ –∫–ª–∞–≤–∏–∞—Ç—É—Ä—É —Å –∫–Ω–æ–ø–∫–æ–π "–ß—Ç–æ —ç—Ç–æ –∑–∞ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ" –ø–æ—Å–ª–µ –æ—Ç–≤–µ—Ç–∞
    markup = ReplyKeyboardRemove()  # Remove the keyboard
    # –°–æ–∑–¥–∞–π—Ç–µ –º–µ–Ω—é —Å –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–º–∏ –∫–Ω–æ–ø–∫–∞–º–∏
    menu_markup = InlineKeyboardMarkup()
    menu_button1 = InlineKeyboardButton("–û—Å–Ω–æ–≤–Ω–æ–µ –º–µ–Ω—é", callback_data="main_menu")
    menu_markup.add(menu_button1)

    # –û—Ç–ø—Ä–∞–≤—å—Ç–µ –æ—Ç–≤–µ—Ç –ø–æ—Å–ª–µ –Ω–∞–∂–∞—Ç–∏—è –∫–Ω–æ–ø–∫–∏ "–•–æ—á—É —Å–∫–æ—Ä–µ–µ –≤–æ—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è —Ç–æ–±–æ–π" —Å –∫–ª–∞–≤–∏–∞—Ç—É—Ä–æ–π —Å –∫–Ω–æ–ø–∫–æ–π
    await message.reply(
        "–ö–æ–Ω–µ—á–Ω–æ, –≤—ã—Å—ã–ª–∞–π —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏—é! üòä",
        reply_markup=markup  # Use the ReplyKeyboardRemove to remove the keyboard
    )
    await UserStates.WaitingForPhoto.set()  # –ü–µ—Ä–µ–≤–æ–¥–∏–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –æ–∂–∏–¥–∞–Ω–∏—è —Ñ–æ—Ç–æ


@dp.message_handler(content_types=['photo'], state=UserStates.WaitingForPhoto)
async def send_drugs(message: types.Message, state: FSMContext):
    await UserStates.AnalyzingPhoto.set()
    
    user_id = message.from_user.id  
    user_first_name = message.from_user.first_name                              
    user_name = message.from_user.username
    
    # Get the file id from the message
    if message.content_type == 'photo':
        file_id = message.photo[-1].file_id
    elif message.content_type == 'document':
        file_id = message.document.file_id

    logging.info(f'{user_first_name}, {user_name} sent img at {time.asctime()}')

    # Get the File object from the bot
    try:
        file = await bot.get_file(file_id)
    except Exception as e:
        logging.error(f'Failed to get File object: {e}')
        return

    # Check if the File object is valid
    if not file:
        logging.error(f'File does not exist or is not accessible')
        return

    # Generate a unique filename for the downloaded file
    unique_filename = str(uuid.uuid4()) + '.jpg'

    # Download the file using the File object
    try:
        await bot.download_file(file.file_path, unique_filename)
    except Exception as e:
        logging.error(f'Failed to download file: {e}')
        return

    # Open the image using the same file name
    img = Image.open(unique_filename)
    # logging.info(f'Model got this: {type(img)}')
    results = model(img)
    detected_labels = [results.names[int(pred[-1])] for pred in results.pred[0]]
    detected_labels = list(set(detected_labels))


    logging.info(f'–ë—ã–ª–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã: {detected_labels}')
    if len(detected_labels)==1:
        pass
    elif len(detected_labels)>=2:
        detected_labels = ['postacne' if x in ['papula', 'pustula'] else x for x in detected_labels]
        detected_labels = list(set(detected_labels))    

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–∫ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è
    await state.update_data(detected_labels=detected_labels)

    await UserStates.ShowingResults.set()

    result = ''
    if len(detected_labels)==1:
        if detected_labels[0] != 'hyperceratos':
            if detected_labels[0] in ['papula', 'postacne', 'pustula']:
                result += '\n\n–£ –í–∞—Å –ø–∞–ø—É–ª—ã/–ø–æ—Å—Ç–∞–∫–Ω–µ/–ø—É—Å—Ç—É–ª—ã ‚ÄºÔ∏è'
                csv_file = "datasets/clean/acne_clean_with_categories.csv"
            elif detected_labels[0] == "cuperos":
                result += '\n\n–£ –í–∞—Å –µ—Å—Ç—å –∫—É–ø–µ—Ä–æ–∑ ‚ÄºÔ∏è'
                csv_file = "datasets/clean/cuperoz_clean_with_categories.csv" 
            elif detected_labels[0] == 'camedon':
                result += '\n\n–£ –í–∞—Å –µ—Å—Ç—å –∫–∞–º–µ–¥–æ–Ω—ã ‚ÄºÔ∏è'
                csv_file = "datasets/clean/camedons_clean_with_categories.csv"

            try:
                data = pd.read_csv(csv_file)
                drug_names = {}
                drug_prices = {}
                drug_ratings = {}

                for x in data['category'].unique():
                    category_data = data[data['category'] == x]
                    top_3_names = category_data['drug_name'][:3].tolist()
                    top_3_prices = category_data['price'][:3].tolist()
                    top_3_ratings = category_data['rating'][:3].tolist()
                    
                    drug_names[x] = top_3_names
                    drug_prices[x] = top_3_prices
                    drug_ratings[x] = top_3_ratings

                result += "\n\n–í–∞–º —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–æ:"

                for category, names, prices, ratings in zip(drug_names.keys(), drug_names.values(), drug_prices.values(), drug_ratings.values()):
                    smiley = category_smileys.get(category, "üòÉ")  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è "üòÉ"
                    result += f"\n\n{smiley} –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {category}"
                    for i, (name, price, rating) in enumerate(zip(names, prices, ratings), start=1):
                        result += f'\n{i}. {name} (—Ä–µ–π—Ç–∏–Ω–≥: {rating}, —Ü–µ–Ω–∞: {price}–†)'

                    
            except Exception as ex:
                result += f'\n–û—à–∏–±–∫–∞: {ex}'

        else:
            result += '\n\n–£ –í–∞—Å –µ—Å—Ç—å –≥–∏–ø–µ—Ä–∫–µ—Ä–∞—Ç–æ–∑ ‚ÄºÔ∏è'
            csv_file = 'datasets/clean/kislots_and_pilings.csv'
            data = pd.read_csv(csv_file)
            kislots_names = data[data['category'] =='–∫–∏—Å–ª–æ—Ç–∞']['drug_name'].tolist()
            pilings_names = data[data['category'] =='–ø–∏–ª–∏–Ω–≥']['drug_name'].tolist()

            try: 
                top_3_kislots = kislots_names[:3]
                top_3_pilings = pilings_names[:3]

                top_3_kislots_r = data[data['category'] =='–∫–∏—Å–ª–æ—Ç–∞']['rating'].tolist()[:3]
                top_3_kislots_p = data[data['category'] =='–∫–∏—Å–ª–æ—Ç–∞']['price'].tolist()[:3]

                top_3_pilings_r = data[data['category'] =='–ø–∏–ª–∏–Ω–≥']['rating'].tolist()[:3]
                top_3_pilings_p = data[data['category'] =='–ø–∏–ª–∏–Ω–≥']['price'].tolist()[:3]

                smiley = category_smileys.get('–∫–∏—Å–ª–æ—Ç–∞', "üòÉ")
                result += f'\n\n{smiley} –°–æ–≤–µ—Ç—É–µ–º –æ–¥–Ω—É –∏–∑ –∫–∏—Å–ª–æ—Ç:'
                for i, (name, rating, price) in enumerate(zip(top_3_kislots, top_3_kislots_r, top_3_kislots_p), start=1):
                    result += f'\n{i}. {name} (—Ä–µ–π—Ç–∏–Ω–≥: {rating}, —Ü–µ–Ω–∞: {price}–†)'
                
                smiley = category_smileys.get('–ø–∏–ª–∏–Ω–≥', "üòÉ")
                result += f'\n\n{smiley} –ò–∑ –ø–∏–ª–∏–Ω–≥–æ–≤:'
                for i, (name, rating, price) in enumerate(zip(top_3_pilings, top_3_pilings_r, top_3_pilings_p), start=1):
                    result += f'\n{i}. {name} (—Ä–µ–π—Ç–∏–Ω–≥: {rating}, —Ü–µ–Ω–∞: {price}–†)'
                    
            except Exception as ex:
                result += f'\n–û—à–∏–±–∫–∞: {ex}'


    elif len(detected_labels)>=2:
        # detected_labels = ['postacne' if x in ['papula', 'pustula'] else x for x in detected_labels]
        # detected_labels = list(set(detected_labels))
        for problem in detected_labels:
            if problem != 'hyperceratos':
                if problem in ['papula', 'postacne', 'pustula']:
                    result += '\n\n–£ –í–∞—Å –ø–∞–ø—É–ª—ã/–ø–æ—Å—Ç–∞–∫–Ω–µ/–ø—É—Å—Ç—É–ª—ã ‚ÄºÔ∏è'
                    csv_file = "datasets/clean/acne_clean_with_categories.csv"
                elif problem == "cuperos":
                    result += '\n\n –£ –í–∞—Å –µ—Å—Ç—å –∫—É–ø–µ—Ä–æ–∑ ‚ÄºÔ∏è'
                    csv_file = "datasets/clean/cuperoz_clean_with_categories.csv" 
                elif problem == 'camedon':
                    result += '\n\n–£ –í–∞—Å –µ—Å—Ç—å –∫–∞–º–µ–¥–æ–Ω—ã ‚ÄºÔ∏è'
                    csv_file = "datasets/clean/camedons_clean_with_categories.csv"

                try:
                    data = pd.read_csv(csv_file)
                    drug_names = {}
                    drug_prices = {}
                    drug_ratings = {}

                    for x in data['category'].unique():
                        category_data = data[data['category'] == x]
                        top_3_names = category_data['drug_name'][:3].tolist()
                        top_3_prices = category_data['price'][:3].tolist()
                        top_3_ratings = category_data['rating'][:3].tolist()
                        
                        drug_names[x] = top_3_names
                        drug_prices[x] = top_3_prices
                        drug_ratings[x] = top_3_ratings

                    result += "\n\n–í–∞–º —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–æ:"

                    for category, names, prices, ratings in zip(drug_names.keys(), drug_names.values(), drug_prices.values(), drug_ratings.values()):
                        smiley = category_smileys.get(category, "üòÉ")  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è "üòÉ"
                        result += f"\n\n{smiley} –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {category}"
                        for i, (name, price, rating) in enumerate(zip(names, prices, ratings), start=1):
                            result += f'\n{i}. {name} (—Ä–µ–π—Ç–∏–Ω–≥: {rating}, —Ü–µ–Ω–∞: {price}–†)'
                    
                except Exception as ex:
                    result += f'\n–û—à–∏–±–∫–∞: {ex}'
                    
            else:
                result += '\n\n–£ –í–∞—Å –µ—Å—Ç—å –≥–∏–ø–µ—Ä–∫–µ—Ä–∞—Ç–æ–∑ ‚ÄºÔ∏è'
                csv_file = 'datasets/clean/kislots_and_pilings.csv'
                data = pd.read_csv(csv_file)
                kislots_names = data[data['category'] =='–∫–∏—Å–ª–æ—Ç–∞']['drug_name'].tolist()
                pilings_names = data[data['category'] =='–ø–∏–ª–∏–Ω–≥']['drug_name'].tolist()

                try: 
                    top_3_kislots = kislots_names[:3]
                    top_3_pilings = pilings_names[:3]

                    top_3_kislots_r = data[data['category'] =='–∫–∏—Å–ª–æ—Ç–∞']['rating'].tolist()[:3]
                    top_3_kislots_p = data[data['category'] =='–∫–∏—Å–ª–æ—Ç–∞']['price'].tolist()[:3]

                    top_3_pilings_r = data[data['category'] =='–ø–∏–ª–∏–Ω–≥']['rating'].tolist()[:3]
                    top_3_pilings_p = data[data['category'] =='–ø–∏–ª–∏–Ω–≥']['price'].tolist()[:3]
                    
                    smiley = category_smileys.get('–∫–∏—Å–ª–æ—Ç–∞', "üòÉ")
                    result += f'\n\n{smiley} –°–æ–≤–µ—Ç—É–µ–º –æ–¥–Ω—É –∏–∑ –∫–∏—Å–ª–æ—Ç:'
                    for i, (name, rating, price) in enumerate(zip(top_3_kislots, top_3_kislots_r, top_3_kislots_p), start=1):
                        result += f'\n{i}. {name} (—Ä–µ–π—Ç–∏–Ω–≥: {rating}, —Ü–µ–Ω–∞: {price}–†)'
                    
                    smiley = category_smileys.get('–ø–∏–ª–∏–Ω–≥', "üòÉ")
                    result += f'\n\n{smiley} –ò–∑ –ø–∏–ª–∏–Ω–≥–æ–≤:'
                    for i, (name, rating, price) in enumerate(zip(top_3_pilings, top_3_pilings_r, top_3_pilings_p), start=1):
                        result += f'\n{i}. {name} (—Ä–µ–π—Ç–∏–Ω–≥: {rating}, —Ü–µ–Ω–∞: {price}–†)'
                        
                except Exception as ex:
                    result += f'\n–û—à–∏–±–∫–∞: {ex}'
    else:
        result = '–£ –≤–∞—Å –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ –ø—Ä–æ–±–ª–µ–º–æ–∫, –≤—Å–µ —Ö–æ—Ä–æ—à–æ ü¶Ñ'
        await UserStates.WaitingForPhoto.set()
    await bot.send_message(user_id, result)

    try:
        os.remove(unique_filename)
        logging.info(f'File {unique_filename} has been deleted.')
    except Exception as e:
        logging.error(f'Failed to delete file: {e}')

    if result != '–£ –≤–∞—Å –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ –ø—Ä–æ–±–ª–µ–º–æ–∫, –≤—Å–µ —Ö–æ—Ä–æ—à–æ ü¶Ñ':
        # –°–æ–∑–¥–∞–Ω–∏–µ inline-–∫–Ω–æ–ø–æ–∫
        get_reviews_button = InlineKeyboardButton(text="–°—É–º–º–∞—Ä–∏–∑–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–∑—ã–≤—ã –Ω–∞ –ø—Ä–µ–ø–∞—Ä–∞—Ç—ã", callback_data="get_reviews")
        not_summarize_button = InlineKeyboardButton(text="–ù–µ —Å—É–º–º–∞—Ä–∏–∑–∏—Ä–æ–≤–∞—Ç—å", callback_data="not_summarize_reviews")
        
        keyboard = InlineKeyboardMarkup().add(get_reviews_button).add(not_summarize_button)  # Arrange buttons vertically
        
        # –û—Ç–ø—Ä–∞–≤–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏—è —Å –∫–Ω–æ–ø–∫–∞–º–∏
        await bot.send_message(user_id, "–í—ã–±–µ—Ä–∏—Ç–µ –¥–µ–π—Å—Ç–≤–∏–µ:", reply_markup=keyboard)
    else:
        pass

# @dp.callback_query_handler(lambda callback_query: callback_query.data in ["get_reviews", "not_summarize_reviews"], state=UserStates.ShowingResults)
# async def handle_review_buttons(callback_query: types.CallbackQuery, state: FSMContext):
#     user_id = callback_query.from_user.id
#     user_data = await state.get_data()

#     if callback_query.data == "get_reviews":
#         await UserStates.ParsingReviews.set()
#         await bot.send_message(user_id, '–ù–µ–º–Ω–æ–≥–æ —Ç–µ—Ä–ø–µ–Ω–∏—è... ‚è≥')

#         detected_labels = user_data.get('detected_labels', [])
#         headers = requests.utils.default_headers()
        
#         if len(detected_labels)==1:
#             detected_labels = detected_labels[0]
#             # await message.reply(detected_labels)
#             if detected_labels != 'hyperceratos':
#                 if detected_labels in ['papula', 'postacne', 'pustula']:
#                     data = pd.read_csv("datasets/clean/acne_clean_with_categories.csv")
#                 elif detected_labels == "cuperos":
#                     data = pd.read_csv("datasets/clean/cuperoz_clean_with_categories.csv")
#                 elif detected_labels == 'camedon':
#                     data = pd.read_csv("datasets/clean/camedons_clean_with_categories.csv")

#                 try:
#                     all_drug_responses = {}
#                     for x in data['category'].unique():
#                         result = ''
#                         category_data = data[data['category'] == x]
#                         top_3_names = category_data['drug_name'][:3].tolist()            
#                         urls = category_data['page_url'][:3]
#                         drug_responses = {}
            
#                         for name, url in zip(top_3_names, urls):
#                             url = 'https://'+url
#                         # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º HTTP –∑–∞–ø—Ä–æ—Å –∏ –ø–æ–ª—É—á–∞–µ–º –æ—Ç–≤–µ—Ç
#                             await asyncio.sleep(1)
#                             r = requests.get(url, headers=headers)
#                             soup = BeautifulSoup(r.text, 'html.parser')
#                             drug_descs = soup.find('div', 'kr_review_plain_text').text
#                             prompt_review  = f'{drug_descs[:3000]} \n\n–í—ã–¥–µ–ª–∏ –≥–ª–∞–≤–Ω—É—é –º—ã—Å–ª—å –∏–∑ –≤—Å–µ—Ö –æ—Ç–∑—ã–≤–æ–≤. –û—Ç–≤–µ—Ç –Ω–∞—á–Ω–∏ —Å–æ —Å–ª–æ–≤: "–≠—Ç–æ—Ç –ø—Ä–µ–ø–∞—Ä–∞—Ç...'
#                             response_text = generate(prompt_review)
#                             drug_responses[name] = response_text
                            
#                         all_drug_responses[x] = drug_responses                  

#                     for cat, drugnamesNreviews in all_drug_responses.items():
#                         smiley = category_smileys.get(cat, "üòÉ")  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è "üòÉ"
#                         result = f"\n\n{smiley} –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {cat}"
#                         for i, (name, drug_r3views) in enumerate(drugnamesNreviews.items(), start=1):
#                             result += f'\n\n{i}. {name}. \n\n{drug_r3views}'
#                         await bot.send_message(user_id, result)                    
                        
#                 except Exception as ex:
#                     result = f'\n–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –æ—Ç–∑—ã–≤–æ–≤ —Å —Å–∞–π—Ç–∞. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑.'
#                     await bot.send_message(user_id, result)

#             else:
#                 data = pd.read_csv('datasets/clean/kislots_and_pilings.csv')
#                 kislots_names = data[data['category'] =='–∫–∏—Å–ª–æ—Ç–∞']['drug_name'].tolist()
#                 pilings_names = data[data['category'] =='–ø–∏–ª–∏–Ω–≥']['drug_name'].tolist()

#                 try:
#                     result = ''
#                     pilings_reviews = {}
#                     kislots_reviews = {}
#                     top_3_kislots = kislots_names[:3]
#                     top_3_pilings = pilings_names[:3]
#                     urls_kislots = data[data['category'] =='–∫–∏—Å–ª–æ—Ç–∞']['page_url'][:3]
#                     urls_pilings = data[data['category'] =='–ø–∏–ª–∏–Ω–≥']['page_url'][:3]

#                     for name, url in zip(top_3_kislots, urls_kislots):
#                         url = 'https://'+url
#                         # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º HTTP –∑–∞–ø—Ä–æ—Å –∏ –ø–æ–ª—É—á–∞–µ–º –æ—Ç–≤–µ—Ç
#                         await asyncio.sleep(1)
#                         r = requests.get(url, headers=headers)
#                         soup = BeautifulSoup(r.text, 'html.parser')
#                         drug_descs = soup.find('div', 'kr_review_plain_text').text
#                         prompt_review  = f'{drug_descs[:4000]} \n\n–í—ã–¥–µ–ª–∏ –≥–ª–∞–≤–Ω—É—é –º—ã—Å–ª—å –∏–∑ –≤—Å–µ—Ö –æ—Ç–∑—ã–≤–æ–≤. –û—Ç–≤–µ—Ç –Ω–∞—á–Ω–∏ —Å–æ —Å–ª–æ–≤: "–≠—Ç–æ—Ç –ø—Ä–µ–ø–∞—Ä–∞—Ç...'
#                         response_text = generate(prompt_review)
#                         kislots_reviews[name] = response_text
                    
#                     # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –∫–∏—Å–ª–æ—Ç
#                     smiley = category_smileys.get('–∫–∏—Å–ª–æ—Ç–∞', "üòÉ")
#                     result = f'\n\n{smiley} –ö–∏—Å–ª–æ—Ç—ã:'
#                     for i, (name, reviews) in enumerate(kislots_reviews.items(), start=1):
#                         result += f'\n\n{i}. {name}. \n\n{reviews}'
#                     await bot.send_message(user_id, result)
#                     await bot.send_message(user_id, '\n\n–î—É–º–∞—é... ‚è≥')

#                     for name, url in zip(top_3_pilings, urls_pilings):
#                         url = 'https://'+url
#                         # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º HTTP –∑–∞–ø—Ä–æ—Å –∏ –ø–æ–ª—É—á–∞–µ–º –æ—Ç–≤–µ—Ç
#                         await asyncio.sleep(1)
#                         r = requests.get(url, headers=headers)
#                         soup = BeautifulSoup(r.text, 'html.parser')
#                         drug_descs = soup.find('div', 'kr_review_plain_text').text
#                         prompt_review  = f'{drug_descs[:4000]} \n\n–í—ã–¥–µ–ª–∏ –≥–ª–∞–≤–Ω—É—é –º—ã—Å–ª—å –∏–∑ –≤—Å–µ—Ö –æ—Ç–∑—ã–≤–æ–≤. –û—Ç–≤–µ—Ç –Ω–∞—á–Ω–∏ —Å–æ —Å–ª–æ–≤: "–≠—Ç–æ—Ç –ø—Ä–µ–ø–∞—Ä–∞—Ç...'
#                         response_text = generate(prompt_review)
#                         pilings_reviews[name] = response_text

#                     # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –ø–∏–ª–∏–Ω–≥–æ–≤
#                     smiley = category_smileys.get('–ø–∏–ª–∏–Ω–≥', "üòÉ")
#                     result = f'\n\n{smiley} –ü–∏–ª–∏–Ω–≥–∏:'
#                     for i, (name, reviews) in enumerate(pilings_reviews.items(), start=1):
#                         result += f'\n\n{i}. {name}. \n\n{reviews}'
#                     await bot.send_message(user_id, result)

#                 except Exception as ex:
#                     result = f'\n–û—à–∏–±–∫–∞ 2: {ex}'
#                     await bot.send_message(user_id, result)


#         elif len(detected_labels)>=2:
#             detected_labels = ['postacne' if x in ['papula', 'pustula'] else x for x in detected_labels]
#             for problem in detected_labels:
#                 if problem != 'hyperceratos':
#                     if problem in ['papula', 'postacne', 'pustula']:
#                         data = pd.read_csv("datasets/clean/acne_clean_with_categories.csv")
#                     elif problem == "cuperos":
#                         data = pd.read_csv("datasets/clean/cuperoz_clean_with_categories.csv")
#                     elif problem == 'camedon':
#                         data = pd.read_csv("datasets/clean/camedons_clean_with_categories.csv")

#                     try:
#                         all_drug_responses = {}
#                         for x in data['category'].unique():
#                             result = ''
#                             drug_responses = {}
#                             category_data = data[data['category'] == x]
#                             urls = category_data['page_url'][:3]
#                             top_3_names = category_data['drug_name'][:3].tolist()

#                             for name, url in zip(top_3_names, urls):
#                                 url = 'https://'+url
#                             # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º HTTP –∑–∞–ø—Ä–æ—Å –∏ –ø–æ–ª—É—á–∞–µ–º –æ—Ç–≤–µ—Ç
#                                 await asyncio.sleep(1)
#                                 r = requests.get(url, headers=headers)
#                                 soup = BeautifulSoup(r.text, 'html.parser')
#                                 drug_descs = soup.find('div', 'kr_review_plain_text').text
#                                 prompt_review  = f'{drug_descs[:4000]} \n\n–í—ã–¥–µ–ª–∏ –≥–ª–∞–≤–Ω—É—é –º—ã—Å–ª—å –∏–∑ –≤—Å–µ—Ö –æ—Ç–∑—ã–≤–æ–≤. –û—Ç–≤–µ—Ç –Ω–∞—á–Ω–∏ —Å–æ —Å–ª–æ–≤: "–≠—Ç–æ—Ç –ø—Ä–µ–ø–∞—Ä–∞—Ç...'
#                                 response_text = generate(prompt_review)
#                                 drug_responses[name] = response_text

#                             all_drug_responses[x] = drug_responses    

#                         for cat, drugnamesNreviews in all_drug_responses.items():
#                             smiley = category_smileys.get(cat, "üòÉ")  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è "üòÉ"
#                             result = f"\n\n{smiley} –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {cat}"
#                             for i, (name, drug_r3views) in enumerate(drugnamesNreviews.items(), start=1):
#                                 result += f'\n\n{i}. {name}. \n\n{drug_r3views}'
#                             await bot.send_message(user_id, result)  
#                             # await message.reply('\n\n–î—É–º–∞—é... ‚è≥')

#                     except Exception as ex:
#                         result = f'\n–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –æ—Ç–∑—ã–≤–æ–≤ —Å —Å–∞–π—Ç–∞. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑.'
#                         await bot.send_message(user_id, result)
                        
#                 else:
#                     data = pd.read_csv('datasets/clean/kislots_and_pilings.csv')
#                     kislots_names = data[data['category'] =='–∫–∏—Å–ª–æ—Ç–∞']['drug_name'].tolist()
#                     pilings_names = data[data['category'] =='–ø–∏–ª–∏–Ω–≥']['drug_name'].tolist()

#                     try:
#                         result = ''
#                         pilings_reviews = {}
#                         kislots_reviews = {}
#                         top_3_kislots = kislots_names[:3]
#                         top_3_pilings = pilings_names[:3]
#                         urls_kislots = data[data['category'] =='–∫–∏—Å–ª–æ—Ç–∞']['page_url'][:3]
#                         urls_pilings = data[data['category'] =='–ø–∏–ª–∏–Ω–≥']['page_url'][:3]

#                         for name, url in zip(top_3_kislots, urls_kislots):
#                             url = 'https://'+url
#                             # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º HTTP –∑–∞–ø—Ä–æ—Å –∏ –ø–æ–ª—É—á–∞–µ–º –æ—Ç–≤–µ—Ç
#                             await asyncio.sleep(1)
#                             r = requests.get(url, headers=headers)
#                             soup = BeautifulSoup(r.text, 'html.parser')
#                             drug_descs = soup.find('div', 'kr_review_plain_text').text
#                             prompt_review  = f'{drug_descs[:4000]} \n\n–í—ã–¥–µ–ª–∏ –≥–ª–∞–≤–Ω—É—é –º—ã—Å–ª—å –∏–∑ –≤—Å–µ—Ö –æ—Ç–∑—ã–≤–æ–≤. –û—Ç–≤–µ—Ç –Ω–∞—á–Ω–∏ —Å–æ —Å–ª–æ–≤: "–≠—Ç–æ—Ç –ø—Ä–µ–ø–∞—Ä–∞—Ç...'
#                             response_text = generate(prompt_review)
#                             kislots_reviews[name] = response_text
                        
#                         # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –∫–∏—Å–ª–æ—Ç
#                         smiley = category_smileys.get('–∫–∏—Å–ª–æ—Ç–∞', "üòÉ")
#                         result = f'\n\n{smiley} –ö–∏—Å–ª–æ—Ç—ã:'
#                         for i, (name, reviews) in enumerate(kislots_reviews.items(), start=1):
#                             result += f'\n\n{i}. {name}. \n\n{reviews}'
#                         await bot.send_message(user_id, result) 
#                         await bot.send_message(user_id, '\n\n–î—É–º–∞—é... ‚è≥')

#                         for name, url in zip(top_3_pilings, urls_pilings):
#                             url = 'https://'+url
#                             # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º HTTP –∑–∞–ø—Ä–æ—Å –∏ –ø–æ–ª—É—á–∞–µ–º –æ—Ç–≤–µ—Ç
#                             await asyncio.sleep(1)
#                             r = requests.get(url, headers=headers)
#                             soup = BeautifulSoup(r.text, 'html.parser')
#                             drug_descs = soup.find('div', 'kr_review_plain_text').text
#                             prompt_review  = f'{drug_descs[:4000]} \n\n–í—ã–¥–µ–ª–∏ –≥–ª–∞–≤–Ω—É—é –º—ã—Å–ª—å –∏–∑ –≤—Å–µ—Ö –æ—Ç–∑—ã–≤–æ–≤. –û—Ç–≤–µ—Ç –Ω–∞—á–Ω–∏ —Å–æ —Å–ª–æ–≤: "–≠—Ç–æ—Ç –ø—Ä–µ–ø–∞—Ä–∞—Ç...'
#                             response_text = generate(prompt_review)
#                             pilings_reviews[name] = response_text

#                         # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –ø–∏–ª–∏–Ω–≥–æ–≤
#                         smiley = category_smileys.get('–ø–∏–ª–∏–Ω–≥', "üòÉ")
#                         result = f'\n\n{smiley} –ü–∏–ª–∏–Ω–≥–∏:'
#                         for i, (name, reviews) in enumerate(pilings_reviews.items(),start=1):
#                             result += f'\n\n{i}. {name}. \n\n{reviews}'
#                         await bot.send_message(user_id, result) 

#                     except Exception as ex:
#                         result = f'\n–û—à–∏–±–∫–∞ 2: {ex}'
#                         await bot.send_message(user_id, result)   

#         await bot.send_message(user_id, '–í–≤–µ–¥–∏—Ç–µ –∫–æ–º–∞–Ω–¥—É /start –¥–ª—è –Ω–∞—á–∞–ª–∞ —Ä–∞–±–æ—Ç—ã')
#         await state.finish()  # –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Å–µ—Å—Å–∏–∏ —Å–æ—Å—Ç–æ—è–Ω–∏–π –ø–æ—Å–ª–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –¥–µ–π—Å—Ç–≤–∏—è

#     elif callback_query.data == "not_summarize_reviews":
#         await bot.send_message(user_id, '–í–≤–µ–¥–∏—Ç–µ –∫–æ–º–∞–Ω–¥—É /start –¥–ª—è –Ω–∞—á–∞–ª–∞ —Ä–∞–±–æ—Ç—ã')
#         await state.finish()  # Reset the state to its initial state
#         # Send a message or perform any other desired actions when "–ù–µ —Å—É–º–º–∞—Ä–∏–∑–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–∑—ã–≤—ã –Ω–∞ –ø—Ä–µ–ø–∞—Ä–∞—Ç—ã" is clicked

#     # await callback_query.answer()  # Acknowledge the button press

def auto_chunk_comments(comments, max_chunk_length):
    """
    Divide comments into equal-sized chunks, ensuring each chunk ends at a "\r\n" boundary.

    Args:
    comments (str): The text containing comments separated by "\r\n".
    max_chunk_length (int): The maximum length for each chunk.

    Returns:
    List[str]: A list of equal-sized chunks.
    """
    # Split the comments into individual comments using "\r\n" as the separator
    individual_comments = comments.split("\r\n")

    # Initialize variables to store the chunks and current chunk
    chunks = []
    current_chunk = ""

    # Iterate through individual comments
    for comment in individual_comments:
        # Check if adding the current comment to the current chunk exceeds the maximum length
        if len(current_chunk) + len(comment) + len("\r\n") <= max_chunk_length:
            # Add the comment to the current chunk
            if current_chunk:
                current_chunk += "\r\n"
            current_chunk += comment
        else:
            # If adding the comment exceeds the maximum length, start a new chunk
            chunks.append(current_chunk)
            current_chunk = comment

    # Append the last chunk if it's not empty
    if current_chunk:
        chunks.append(current_chunk)

    return chunks


@dp.callback_query_handler(lambda callback_query: callback_query.data in ["get_reviews", "not_summarize_reviews"], state=UserStates.ShowingResults)
async def handle_review_buttons(callback_query: types.CallbackQuery, state: FSMContext):
    user_id = callback_query.from_user.id
    user_data = await state.get_data()
    all_comments = pd.read_csv('parsed_comments/all_comments.csv')

    if callback_query.data == "get_reviews":
        await UserStates.ParsingReviews.set()
        await bot.send_message(user_id, '–ù–µ–º–Ω–æ–≥–æ —Ç–µ—Ä–ø–µ–Ω–∏—è... ‚è≥')

        detected_labels = user_data.get('detected_labels', [])
        headers = requests.utils.default_headers()
        
        if len(detected_labels)==1:
            detected_labels = detected_labels[0]
            # await message.reply(detected_labels)
            if detected_labels != 'hyperceratos':
                if detected_labels in ['papula', 'postacne', 'pustula']:
                    data = pd.read_csv("datasets/clean/acne_clean_with_categories.csv")
                elif detected_labels == "cuperos":
                    data = pd.read_csv("datasets/clean/cuperoz_clean_with_categories.csv")
                elif detected_labels == 'camedon':
                    data = pd.read_csv("datasets/clean/camedons_clean_with_categories.csv")

                try:
                    all_drug_responses = {}
                    for x in data['category'].unique():
                        result = ''
                        category_data = data[data['category'] == x]
                        top_3_names = category_data['drug_name'][:3].tolist()            
                        urls = category_data['page_url'][:3]
                        drug_responses = {}

                        for name, url in zip(top_3_names, urls):
                            # Combine all comments for a particular drug
                            drug_descs = all_comments.loc[all_comments['Drug Name'] == name]['Comments'].str.cat(sep='\r\n')[:30000]

                            # Chunk the combined comments using the auto_chunk_comments function
                            chunks = auto_chunk_comments(drug_descs, 3000)  # Assuming 3000 is the max chunk length

                            # Generate summary for each chunk
                            chunk_summaries = []
                            for chunk in chunks:
                                chunk_prompt = f'{chunk[:3000]} \n\n–í—ã–¥–µ–ª–∏ –≥–ª–∞–≤–Ω—É—é –º—ã—Å–ª—å –∏–∑ —ç—Ç–æ–≥–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ –æ—Ç–∑—ã–≤–æ–≤. –û—Ç–≤–µ—Ç –Ω–∞—á–Ω–∏ —Å–æ —Å–ª–æ–≤: "–≠—Ç–æ—Ç —Ñ—Ä–∞–≥–º–µ–Ω—Ç –æ—Ç–∑—ã–≤–æ–≤ –≥–æ–≤–æ—Ä–∏—Ç –æ —Ç–æ–º, —á—Ç–æ..."'
                                chunk_summary = generate(chunk_prompt)  # Replace 'generate' with your summarization function
                                chunk_summaries.append(chunk_summary)

                            # Combine chunk summaries to create a global summary
                            combined_chunk_summaries = ' '.join(chunk_summaries)
                            global_summary_prompt = f'{combined_chunk_summaries[:3000]} \n\n–°—É–º–º–∏—Ä—É–π –æ–±—â—É—é –º—ã—Å–ª—å –≤—Å–µ—Ö —ç—Ç–∏—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤. –û—Ç–≤–µ—Ç –Ω–∞—á–Ω–∏ —Å–æ —Å–ª–æ–≤: "–≠—Ç–æ—Ç –ø—Ä–µ–ø–∞—Ä–∞—Ç..."'
                            global_summary = generate(global_summary_prompt)

                            # Store the global summary for the drug
                            drug_responses[name] = global_summary
                                
                        all_drug_responses[x] = drug_responses                  

                    for cat, drugnamesNreviews in all_drug_responses.items():
                        smiley = category_smileys.get(cat, "üòÉ")  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è "üòÉ"
                        result = f"\n\n{smiley} –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {cat}"
                        for i, (name, drug_r3views) in enumerate(drugnamesNreviews.items(), start=1):
                            result += f'\n\n{i}. {name}. \n\n{drug_r3views}'
                        await bot.send_message(user_id, result)                    
                        
                except Exception as ex:
                    result = f'\n–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –æ—Ç–∑—ã–≤–æ–≤ —Å —Å–∞–π—Ç–∞. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑.'
                    await bot.send_message(user_id, result)

            else:
                data = pd.read_csv('datasets/clean/kislots_and_pilings.csv')
                kislots_names = data[data['category'] == '–∫–∏—Å–ª–æ—Ç–∞']['drug_name'].tolist()
                pilings_names = data[data['category'] == '–ø–∏–ª–∏–Ω–≥']['drug_name'].tolist()

                try:
                    result = ''
                    pilings_reviews = {}
                    kislots_reviews = {}
                    top_3_kislots = kislots_names[:3]
                    top_3_pilings = pilings_names[:3]
                    urls_kislots = data[data['category'] == '–∫–∏—Å–ª–æ—Ç–∞']['page_url'][:3]
                    urls_pilings = data[data['category'] == '–ø–∏–ª–∏–Ω–≥']['page_url'][:3]

                    max_chunk_length = 3000  # Define your maximum chunk length

                    # Process for Kislots
                    for name, url in zip(top_3_kislots, urls_kislots):
                        drug_descs = all_comments.loc[all_comments['Drug Name'] == name]['Comments'].str.cat(sep='\r\n')[:30000]
                        chunks = auto_chunk_comments(drug_descs, max_chunk_length)
                        chunk_summaries = []
                        for chunk in chunks:
                            chunk_prompt = f'{chunk[:3000]} \n\n–í—ã–¥–µ–ª–∏ –≥–ª–∞–≤–Ω—É—é –º—ã—Å–ª—å –∏–∑ —ç—Ç–æ–≥–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ –æ—Ç–∑—ã–≤–æ–≤...'
                            chunk_summary = generate(chunk_prompt)
                            chunk_summaries.append(chunk_summary)
                        combined_chunk_summaries = ' '.join(chunk_summaries)
                        global_summary_prompt = f'{combined_chunk_summaries[:3000]} \n\n–°—É–º–º–∏—Ä—É–π –æ–±—â—É—é –º—ã—Å–ª—å –≤—Å–µ—Ö —ç—Ç–∏—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤...'
                        global_summary = generate(global_summary_prompt)
                        kislots_reviews[name] = global_summary

                    # Output for Kislots
                    smiley = category_smileys.get('–∫–∏—Å–ª–æ—Ç–∞', "üòÉ")
                    result = f'\n\n{smiley} –ö–∏—Å–ª–æ—Ç—ã:'
                    for i, (name, reviews) in enumerate(kislots_reviews.items(), start=1):
                        result += f'\n\n{i}. {name}. \n\n{reviews}'
                    await bot.send_message(user_id, result)

                    # Process for Pilings
                    for name, url in zip(top_3_pilings, urls_pilings):
                        drug_descs = all_comments.loc[all_comments['Drug Name'] == name]['Comments'].str.cat(sep='\r\n')[:30000]
                        chunks = auto_chunk_comments(drug_descs, max_chunk_length)
                        chunk_summaries = []
                        for chunk in chunks:
                            chunk_prompt = f'{chunk[:3000]} \n\n–í—ã–¥–µ–ª–∏ –≥–ª–∞–≤–Ω—É—é –º—ã—Å–ª—å –∏–∑ —ç—Ç–æ–≥–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ –æ—Ç–∑—ã–≤–æ–≤...'
                            chunk_summary = generate(chunk_prompt)
                            chunk_summaries.append(chunk_summary)
                        combined_chunk_summaries = ' '.join(chunk_summaries)
                        global_summary_prompt = f'{combined_chunk_summaries[:3000]} \n\n–°—É–º–º–∏—Ä—É–π –æ–±—â—É—é –º—ã—Å–ª—å –≤—Å–µ—Ö —ç—Ç–∏—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤...'
                        global_summary = generate(global_summary_prompt)
                        pilings_reviews[name] = global_summary

                    # Output for Pilings
                    smiley = category_smileys.get('–ø–∏–ª–∏–Ω–≥', "üòÉ")
                    result = f'\n\n{smiley} –ü–∏–ª–∏–Ω–≥–∏:'
                    for i, (name, reviews) in enumerate(pilings_reviews.items(), start=1):
                        result += f'\n\n{i}. {name}. \n\n{reviews}'
                    await bot.send_message(user_id, result)

                except Exception as ex:
                    result = f'\n–û—à–∏–±–∫–∞ 2: {ex}'
                    await bot.send_message(user_id, result)

        elif len(detected_labels)>=2:
            detected_labels = ['postacne' if x in ['papula', 'pustula'] else x for x in detected_labels]
            for problem in detected_labels:
                if problem != 'hyperceratos':
                    if problem in ['papula', 'postacne', 'pustula']:
                        data = pd.read_csv("datasets/clean/acne_clean_with_categories.csv")
                    elif problem == "cuperos":
                        data = pd.read_csv("datasets/clean/cuperoz_clean_with_categories.csv")
                    elif problem == 'camedon':
                        data = pd.read_csv("datasets/clean/camedons_clean_with_categories.csv")

                    try:
                        all_drug_responses = {}
                        for x in data['category'].unique():
                            result = ''
                            drug_responses = {}
                            category_data = data[data['category'] == x]
                            urls = category_data['page_url'][:3]
                            top_3_names = category_data['drug_name'][:3].tolist()

                            for name, url in zip(top_3_names, urls):
                                # Combine all comments for a particular drug
                                drug_descs = all_comments.loc[all_comments['Drug Name'] == name]['Comments'].str.cat(sep='\r\n')[:30000]

                                # Chunk the combined comments using the auto_chunk_comments function
                                chunks = auto_chunk_comments(drug_descs, 3000)  # Assuming 3000 is the max chunk length

                                # Generate summary for each chunk
                                chunk_summaries = []
                                for chunk in chunks:
                                    chunk_prompt = f'{chunk[:3000]} \n\n–í—ã–¥–µ–ª–∏ –≥–ª–∞–≤–Ω—É—é –º—ã—Å–ª—å –∏–∑ —ç—Ç–æ–≥–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ –æ—Ç–∑—ã–≤–æ–≤. –û—Ç–≤–µ—Ç –Ω–∞—á–Ω–∏ —Å–æ —Å–ª–æ–≤: "–≠—Ç–æ—Ç —Ñ—Ä–∞–≥–º–µ–Ω—Ç –æ—Ç–∑—ã–≤–æ–≤ –≥–æ–≤–æ—Ä–∏—Ç –æ —Ç–æ–º, —á—Ç–æ..."'
                                    chunk_summary = generate(chunk_prompt)  # Replace 'generate' with your summarization function
                                    chunk_summaries.append(chunk_summary)

                                # Combine chunk summaries to create a global summary
                                combined_chunk_summaries = ' '.join(chunk_summaries)
                                global_summary_prompt = f'{combined_chunk_summaries[:3000]} \n\n–°—É–º–º–∏—Ä—É–π –æ–±—â—É—é –º—ã—Å–ª—å –≤—Å–µ—Ö —ç—Ç–∏—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤. –û—Ç–≤–µ—Ç –Ω–∞—á–Ω–∏ —Å–æ —Å–ª–æ–≤: "–≠—Ç–æ—Ç –ø—Ä–µ–ø–∞—Ä–∞—Ç..."'
                                global_summary = generate(global_summary_prompt)

                                # Store the global summary for the drug
                                drug_responses[name] = global_summary
                                    
                        all_drug_responses[x] = drug_responses       

                        for cat, drugnamesNreviews in all_drug_responses.items():
                            smiley = category_smileys.get(cat, "üòÉ")  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è "üòÉ"
                            result = f"\n\n{smiley} –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {cat}"
                            for i, (name, drug_r3views) in enumerate(drugnamesNreviews.items(), start=1):
                                result += f'\n\n{i}. {name}. \n\n{drug_r3views}'
                            await bot.send_message(user_id, result)  
                            # await message.reply('\n\n–î—É–º–∞—é... ‚è≥')

                    except Exception as ex:
                        result = f'\n–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –æ—Ç–∑—ã–≤–æ–≤ —Å —Å–∞–π—Ç–∞. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑.'
                        await bot.send_message(user_id, result)
                        
                else:
                    data = pd.read_csv('datasets/clean/kislots_and_pilings.csv')
                    kislots_names = data[data['category'] =='–∫–∏—Å–ª–æ—Ç–∞']['drug_name'].tolist()
                    pilings_names = data[data['category'] =='–ø–∏–ª–∏–Ω–≥']['drug_name'].tolist()

                    try:
                        result = ''
                        pilings_reviews = {}
                        kislots_reviews = {}
                        top_3_kislots = kislots_names[:3]
                        top_3_pilings = pilings_names[:3]
                        urls_kislots = data[data['category'] =='–∫–∏—Å–ª–æ—Ç–∞']['page_url'][:3]
                        urls_pilings = data[data['category'] =='–ø–∏–ª–∏–Ω–≥']['page_url'][:3]
                        
                        # Process for Kislots
                        for name, url in zip(top_3_kislots, urls_kislots):
                            drug_descs = all_comments.loc[all_comments['Drug Name'] == name]['Comments'].str.cat(sep='\r\n')[:30000]
                            chunks = auto_chunk_comments(drug_descs, max_chunk_length)
                            chunk_summaries = []
                            for chunk in chunks:
                                chunk_prompt = f'{chunk[:3000]} \n\n–í—ã–¥–µ–ª–∏ –≥–ª–∞–≤–Ω—É—é –º—ã—Å–ª—å –∏–∑ —ç—Ç–æ–≥–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ –æ—Ç–∑—ã–≤–æ–≤...'
                                chunk_summary = generate(chunk_prompt)
                                chunk_summaries.append(chunk_summary)
                            combined_chunk_summaries = ' '.join(chunk_summaries)
                            global_summary_prompt = f'{combined_chunk_summaries[:3000]} \n\n–°—É–º–º–∏—Ä—É–π –æ–±—â—É—é –º—ã—Å–ª—å –≤—Å–µ—Ö —ç—Ç–∏—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤...'
                            global_summary = generate(global_summary_prompt)
                            kislots_reviews[name] = global_summary
                        
                        # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –∫–∏—Å–ª–æ—Ç
                        smiley = category_smileys.get('–∫–∏—Å–ª–æ—Ç–∞', "üòÉ")
                        result = f'\n\n{smiley} –ö–∏—Å–ª–æ—Ç—ã:'
                        for i, (name, reviews) in enumerate(kislots_reviews.items(), start=1):
                            result += f'\n\n{i}. {name}. \n\n{reviews}'
                        await bot.send_message(user_id, result) 
                        await bot.send_message(user_id, '\n\n–î—É–º–∞—é... ‚è≥')

                        # Process for Pilings
                        for name, url in zip(top_3_pilings, urls_pilings):
                            drug_descs = all_comments.loc[all_comments['Drug Name'] == name]['Comments'].str.cat(sep='\r\n')[:30000]
                            chunks = auto_chunk_comments(drug_descs, max_chunk_length)
                            chunk_summaries = []
                            for chunk in chunks:
                                chunk_prompt = f'{chunk[:3000]} \n\n–í—ã–¥–µ–ª–∏ –≥–ª–∞–≤–Ω—É—é –º—ã—Å–ª—å –∏–∑ —ç—Ç–æ–≥–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ –æ—Ç–∑—ã–≤–æ–≤...'
                                chunk_summary = generate(chunk_prompt)
                                chunk_summaries.append(chunk_summary)
                            combined_chunk_summaries = ' '.join(chunk_summaries)
                            global_summary_prompt = f'{combined_chunk_summaries[:3000]} \n\n–°—É–º–º–∏—Ä—É–π –æ–±—â—É—é –º—ã—Å–ª—å –≤—Å–µ—Ö —ç—Ç–∏—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤...'
                            global_summary = generate(global_summary_prompt)
                            pilings_reviews[name] = global_summary

                        # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –ø–∏–ª–∏–Ω–≥–æ–≤
                        smiley = category_smileys.get('–ø–∏–ª–∏–Ω–≥', "üòÉ")
                        result = f'\n\n{smiley} –ü–∏–ª–∏–Ω–≥–∏:'
                        for i, (name, reviews) in enumerate(pilings_reviews.items(),start=1):
                            result += f'\n\n{i}. {name}. \n\n{reviews}'
                        await bot.send_message(user_id, result) 

                    except Exception as ex:
                        result = f'\n–û—à–∏–±–∫–∞ 2: {ex}'
                        await bot.send_message(user_id, result)   

        await bot.send_message(user_id, '–í–≤–µ–¥–∏—Ç–µ –∫–æ–º–∞–Ω–¥—É /start –¥–ª—è –Ω–∞—á–∞–ª–∞ —Ä–∞–±–æ—Ç—ã')
        await state.finish()  # –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Å–µ—Å—Å–∏–∏ —Å–æ—Å—Ç–æ—è–Ω–∏–π –ø–æ—Å–ª–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –¥–µ–π—Å—Ç–≤–∏—è

    elif callback_query.data == "not_summarize_reviews":
        await bot.send_message(user_id, '–í–≤–µ–¥–∏—Ç–µ –∫–æ–º–∞–Ω–¥—É /start –¥–ª—è –Ω–∞—á–∞–ª–∞ —Ä–∞–±–æ—Ç—ã')
        await state.finish()  # Reset the state to its initial state
        # Send a message or perform any other desired actions when "–ù–µ —Å—É–º–º–∞—Ä–∏–∑–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–∑—ã–≤—ã –Ω–∞ –ø—Ä–µ–ø–∞—Ä–∞—Ç—ã" is clicked

    # await callback_query.answer()  # Acknowledge the button press


if __name__ == '__main__':
    from aiogram import executor
    executor.start_polling(dp, skip_updates=True)
